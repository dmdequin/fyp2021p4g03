{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Year Project\n",
    "## Project 4 - Natural Language Processing\n",
    "### Professor - Christian Hardmeier\n",
    "\n",
    "This notebook contains all of the code developed for project 4. We will be using a data set of tweets to perform machine learning for binary and multiclass classification.\n",
    "\n",
    "For **binary** classification, we evaluate tweets based on *'ironic'* or *'not ironic'*. [Learn More](https://www.aclweb.org/anthology/S18-1005.pdf)\n",
    "<br>\n",
    "For **multiclass** classification, we evaluate predict which emojis are used based on the text data. [Learn More](https://www.aclweb.org/anthology/S18-1003.pdf)\n",
    "\n",
    "Group 3:<br>\n",
    "Crisanna Cornish (ccor@itu.dk)<br>\n",
    "Danielle Dequin (ddeq@itu.dk)<br>\n",
    "Gino Franco Fazzi (gifa@itu.dk)<br>\n",
    "Moneeca Abru Iftikhar Latif (abml@itu.dk)<br>\n",
    "Carl August Wismer (cwis@itu.dk)\n",
    "\n",
    "Created: 27-04-2021<br>\n",
    "Last Modified: 2-06-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "We use the TweetEval repository, a collection of 7 datasets for different classification tasks based on social media post. The repository can be found here: https://github.com/cardiffnlp/tweeteval.git\n",
    "\n",
    "Each dataset is presented in the same format and with fixed training, validation and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import NgramCounter\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Laplace\n",
    "from nltk.lm import KneserNeyInterpolated\n",
    "from nltk.lm import WittenBellInterpolated\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "\n",
    "from sklearn.metrics import classification_report,\\\n",
    "confusion_matrix, accuracy_score, plot_confusion_matrix, recall_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irony text:\n",
    "IRONY_RAW_PATH = '../datasets/irony/raw/'\n",
    "IRONY_INTERIM_PATH = '../datasets/irony/interim/'\n",
    "\n",
    "# emoji:\n",
    "EMOJI_RAW_PATH = '../datasets/emoji/raw/'\n",
    "EMOJI_INTERIM_PATH = '../datasets/emoji/interim/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = 'test_text.txt'\n",
    "TRAIN = 'train_text.txt'\n",
    "VAL = 'val_text.txt'\n",
    "\n",
    "TEST_LABELS = 'test_labels.txt'\n",
    "TRAIN_LABELS = 'train_labels.txt'\n",
    "VAL_LABELS = 'val_labels.txt'\n",
    "\n",
    "TEST_INTERIM = 'test_separated.csv'\n",
    "TRAIN_INTERIM = 'train_separated.csv'\n",
    "VAL_INTERIM = 'val_separated.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(line):\n",
    "    \"\"\"A function that takes input and tokenizes the input\n",
    "    \"\"\"\n",
    "    # Initialise lists\n",
    "    tokens = []\n",
    "    unmatchable = []\n",
    "\n",
    "    # Compile patterns for speedup\n",
    "    token_pat = re.compile(r'\\w+|#+|\\'|@|\\.\\.+|!+|\\?+')\n",
    "    skippable_pat = re.compile(r',|\\|http://t.co/+')  # typically spaces\n",
    "\n",
    "    # As long as there's any material left...\n",
    "    while line:\n",
    "        # Try finding a skippable token delimiter first.\n",
    "        skippable_match = re.search(skippable_pat, line)\n",
    "        if skippable_match and skippable_match.start() == 0:\n",
    "            # If there is one at the beginning of the line, just skip it.\n",
    "            line = line[skippable_match.end():]\n",
    "        else:\n",
    "            # Else try finding a real token.\n",
    "            token_match = re.search(token_pat, line)\n",
    "            if token_match and token_match.start() == 0:\n",
    "                #print(line[token_match.start():token_match.end()])\n",
    "                if line[token_match.start():token_match.end()] == '#': # keep hash tags together and seperate\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        else:\n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                elif line[token_match.start():token_match.end()] == '@': # keep @ tags together and seperate\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        \n",
    "                        else: \n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                elif line[token_match.start():token_match.end()] == \"'\": # handle contractions as a single word\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        \n",
    "                        else: \n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                # If there is one at the beginning of the line, tokenise it\n",
    "                else:\n",
    "                    tokens.append(line[:token_match.end()])\n",
    "                    line = line[token_match.end():]\n",
    "            else:\n",
    "                # Else there is unmatchable material\n",
    "                # It ends where a skippable or token match starts, or at the end of the line.\n",
    "                unmatchable_end = len(line)\n",
    "                if skippable_match:\n",
    "                    unmatchable_end = skippable_match.start()\n",
    "                if token_match:\n",
    "                    unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "                # Add it to unmatchable and discard from line.\n",
    "                unmatchable.append(line[:unmatchable_end])\n",
    "                line = line[unmatchable_end:]\n",
    "\n",
    "    final_tokens = []\n",
    "\n",
    "    while len(tokens) > 0:\n",
    "        temp1 = tokens.pop(0)\n",
    "        try:\n",
    "            temp2 = tokens.pop(0)\n",
    "            if temp2[0] == \"'\":\n",
    "                temp1 += temp2\n",
    "                final_tokens.insert(0, temp1)\n",
    "            else:\n",
    "                final_tokens.insert(0, temp1)\n",
    "                tokens.insert(0, temp2)\n",
    "        except:\n",
    "            final_tokens.insert(0, temp1)\n",
    "        \n",
    "    final_tokens = final_tokens[::-1]\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "def token_data(data, interim, tokenizer=None):\n",
    "    \"\"\"Function to tokenize from raw text file. Takes a reading file path, a writing file path and a\n",
    "    tokenizer argument (None for default tokenizer, 'Compare' for TweetTokenizer).\n",
    "    Writes a file with the tokenize lines and returns a list (lines) of lists (tokens).\n",
    "    \"\"\"\n",
    "    if tokenizer == None:\n",
    "        # Open raw data text and tokenize using our tokenise function\n",
    "        f = open(data, \"r\", encoding=\"utf-8\")\n",
    "        token_list = []\n",
    "        for line in f:\n",
    "            token_list.append(tokenise(line))\n",
    "        f.close()\n",
    "    \n",
    "    elif tokenizer == \"compare\":\n",
    "        # Use the NLTK TweetTokenizer\n",
    "        tknzr = TweetTokenizer()\n",
    "\n",
    "        f = open(data, \"r\", encoding=\"utf-8\")\n",
    "        token_list = []\n",
    "        for line in f:\n",
    "            token_list.append(tknzr.tokenize(line))\n",
    "        f.close()\n",
    "        \n",
    "    # Write the tokenized data to an interim csv file\n",
    "    with open(interim, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(token_list)\n",
    "        \n",
    "    return token_list\n",
    "\n",
    "def token_counter(tokens, n= 10, inverse=False):\n",
    "    \"\"\"Count Tokens. Default setting will output the 10 most common tokens.\n",
    "    If parameter inverse=True, output the 10 least common tokens.\n",
    "    \"\"\"\n",
    "    plain = [t for l in tokens for t in l]\n",
    "    counter = Counter(plain)\n",
    "    if not inverse:\n",
    "        return counter.most_common(n)\n",
    "    else:\n",
    "        return counter.most_common()[:-n-1:-1]\n",
    "\n",
    "def zipf_law(tokens, ds_name=''):\n",
    "    \"\"\"What do I do?\n",
    "    \"\"\"\n",
    "    x = [log(x) for x in range(1, len(tokens)+1)]\n",
    "    y = []\n",
    "\n",
    "    ordered = token_counter(tokens, n=len(tokens))\n",
    "\n",
    "    for tup in ordered:\n",
    "        word, count = tup\n",
    "        y.append(log(count))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y)\n",
    "    ax.set_xlabel('Log(Rank)')\n",
    "    ax.set_ylabel('Log(Freq)')\n",
    "    ax.set_title(f'Zipf Law for {ds_name} dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data: Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Irony Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Irony Training data set, tokenize, and write to a csv file\n",
    "irony = token_data(IRONY_RAW_PATH + TRAIN, IRONY_INTERIM_PATH+TRAIN_INTERIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "irony[0] # A taste of the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Emoji Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Emoji raw training data, tokenize, and write to an interim csv file\n",
    "emoji = token_data(EMOJI_RAW_PATH + TRAIN, EMOJI_INTERIM_PATH+TRAIN_INTERIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji[2] # A test of emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Tokeniser Output \n",
    "With the baseline tokenisation from the socialmedia tokeniser in the NLTK library (nltk.tokenize.TweetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open emoji training data, tokenize with NLTK tokenizer, write to interim csv for comparison\n",
    "compare_emoji = token_data(EMOJI_RAW_PATH + TRAIN, EMOJI_INTERIM_PATH+'train_separated_compared.csv',\"compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_emoji[3])\n",
    "print(emoji[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open irony training data, tokenize with NLTK tokenizer, write to interim csv for comparison\n",
    "compare_irony = token_data(IRONY_RAW_PATH + TRAIN, IRONY_INTERIM_PATH+'train_separated_compared.csv',\"compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_irony[3])\n",
    "print(irony[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterize Data: Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Irony Training Dataframe\n",
    "temp1 = pd.read_csv(IRONY_INTERIM_PATH+TRAIN_INTERIM, delimiter=\"\\n\", names=['tweet'])\n",
    "temp2 = pd.read_csv(IRONY_RAW_PATH+TRAIN_LABELS, names=['label'])\n",
    "irony_train_df = temp1.merge(temp2, left_index=True, right_index=True)\n",
    "\n",
    "# Load Irony Validation Data\n",
    "irony_val = token_data(IRONY_RAW_PATH + VAL, IRONY_INTERIM_PATH + VAL_INTERIM)\n",
    "\n",
    "# Create Irony Validation Dataframe\n",
    "temp1 = pd.read_csv(IRONY_INTERIM_PATH+VAL_INTERIM, delimiter=\"\\n\", names=['tweet'])\n",
    "temp2 = pd.read_csv(IRONY_RAW_PATH+VAL_LABELS, names=['label'])\n",
    "irony_val_df = temp1.merge(temp2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irony Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {irony_train_df.shape[0]} tweets in the Irony data set\")\n",
    "print(f'Ironic: {irony_train_df.label.value_counts()[1]}\\nNon-ironic: {irony_train_df.label.value_counts()[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter(irony)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter(irony, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_tokens = [t for l in irony for t in l] # Get tokens for each line of the irony data set\n",
    "irony_one_timers = []\n",
    "irony_mult_timers = []\n",
    "\n",
    "for tup in token_counter(irony, n=len(irony_tokens)):\n",
    "    k, v = tup # Unpack\n",
    "    if v == 1:\n",
    "        irony_one_timers.append(k)\n",
    "    else:\n",
    "        irony_mult_timers.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of words that occur only once:\\\n",
    "{len(irony_one_timers)} ({len(irony_one_timers)/(len(irony_one_timers)+len(irony_mult_timers)):.0%})\\n\\\n",
    "Number of words that occur more than once:\\\n",
    "{len(irony_mult_timers)} ({len(irony_mult_timers)/(len(irony_one_timers)+len(irony_mult_timers)):.0%})\\n\\\n",
    "Total word count, including repeated words: {len(irony_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipf_law(irony, 'Irony');\n",
    "#plt.savefig('../reports/figures/zipf_law_irony.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simple visual inspection, the graph seems to be consistent with the empirical Zipf Law (a semi-straight line crosses the plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{Type}{Token} ratio$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of number of types (vocabulary size) to number of tokens\n",
    "(text/corpus size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_counter(irony, n=len(irony_tokens))) / len(irony_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Emoji Validation Dataframe\n",
    "temp1 = pd.read_csv(EMOJI_INTERIM_PATH+TRAIN_INTERIM, delimiter=\"\\n\", names=['tweet'])\n",
    "temp2 = pd.read_csv(EMOJI_RAW_PATH+TRAIN_LABELS, names=['label'])\n",
    "emoji_train_df = temp1.merge(temp2, left_index=True, right_index=True)  \n",
    "\n",
    "# Load Emoji Validation Data\n",
    "emoji_val = token_data(EMOJI_RAW_PATH + VAL, EMOJI_INTERIM_PATH + VAL_INTERIM)\n",
    "\n",
    "# Create Emoji Validation Dataframe\n",
    "temp1 = pd.read_csv(EMOJI_INTERIM_PATH+VAL_INTERIM, delimiter=\"\\n\", names=['tweet'])\n",
    "temp2 = pd.read_csv(EMOJI_RAW_PATH+VAL_LABELS, names=['label'])\n",
    "emoji_val_df = temp1.merge(temp2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {emoji_train_df.shape[0]} Tweets in the Emoji Training data set.\") #should be 45000\n",
    "for i in range(20):\n",
    "    print(f'Emoji {i}: ', (emoji_train_df[emoji_train_df['label'] == i].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_counter(emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter(emoji, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_tokens = [t for l in emoji for t in l] # Get tokens for each line of the emoji data set\n",
    "emoji_one_timers = []\n",
    "emoji_mult_timers = []\n",
    "\n",
    "for tup in token_counter(emoji, n=len(emoji_tokens)):\n",
    "    k, v = tup # Unpack\n",
    "    if v == 1:\n",
    "        emoji_one_timers.append(k)\n",
    "    else:\n",
    "        emoji_mult_timers.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of words that occur only once:\\\n",
    "{len(emoji_one_timers)} ({len(emoji_one_timers)/(len(emoji_one_timers)+len(emoji_mult_timers)):.0%})\\n\\\n",
    "Number of words that occur more than once:\\\n",
    "{len(emoji_mult_timers)} ({len(emoji_mult_timers)/(len(emoji_one_timers)+len(emoji_mult_timers)):.0%})\\n\\\n",
    "Total word count, including repeated words: {len(emoji_tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipf_law(emoji, 'Emoji');\n",
    "#plt.savefig('../reports/figures/zipf_law_emoji.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simple visual inspection, the graph seems to be consistent with the empirical Zipf Law (a semi-straight line crosses the plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{Type}{Token} ratio$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of number of types (vocabulary size) to number of tokens\n",
    "(text/corpus size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_counter(emoji, n=len(emoji_tokens))) / len(emoji_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the vocabulary on emoji tweets is narrower than the one on irony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2?\n",
    "WHAT DO WE DO WITH THIS SECTION???**********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm = MLE(2) # Maximum likelyhood estimator or order 2\n",
    "len(lm.vocab) # Initializes an empty vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(train, vocab) # which is filled with model data\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.vocab) # Ahhh????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.vocab.lookup(irony[0])\n",
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(\"<UNK>\") == lm.score(\"aliens\") # The token 'aliens' is not in our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(\"a\") # returns the relative frequency of 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.logscore(\"a\") # This method avoids underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(\"@user\") # most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the validation set\n",
    "irony_val = token_data(IRONY_RAW_PATH + VAL, IRONY_INTERIM_PATH+VAL_INTERIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm2 = Laplace(1)\n",
    "len(lm2.vocab) # Initializes an empty vocab\n",
    "lm2.fit(train, vocab) # Which is filled with model data\n",
    "len(lm2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.score(\"<UNK>\") == lm.score(\"aliens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.score(\"@user\") # most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm3 = KneserNeyInterpolated(1)\n",
    "print(len(lm3.vocab)) # Initializes an empty vocab\n",
    "lm3.fit(train, vocab) # Which is filled with model data\n",
    "len(lm3.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.score(\"<UNK>\") == lm.score(\"aliens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.score(\"@user\") # most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WittenBellInterpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm4 = WittenBellInterpolated(1)\n",
    "print(len(lm4.vocab)) # Initializes an empty vocab\n",
    "lm4.fit(train, vocab) # which is filled with model data\n",
    "print(len(lm4.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm4.score(\"<UNK>\") == lm.score(\"aliens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm4.score(\"@user\") #most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm4.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm4.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm4.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bit of fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ironic = irony_train_df[irony_train_df['label'] == 1]['tweet'].reset_index().drop('index', axis=1)\n",
    "ironic_list = [t.split(',') for t in ironic['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, ironic_list)\n",
    "lm_ironic = Laplace(1)\n",
    "lm_ironic.fit(train, vocab)\n",
    "lm_ironic.vocab;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm_ironic.generate(12, random_seed=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-Annotator Agreement: Task 3\n",
    "ADD COMMENTS TO THIS SECTION*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rater_list = ['Dee', 'Sanna', 'Gino', 'August', 'Moneeca']\n",
    "\n",
    "# Create annotation csv, and add true labels\n",
    "interrater_df = pd.read_csv('../datasets/iaa-sets/irony/iaa_labels.txt', names = ['True_label'])\n",
    "\n",
    "# Add individual annotator labels\n",
    "for r in rater_list:\n",
    "    raterX = pd.read_csv('../datasets/iaa-sets/irony/'+r+'Annotation.txt', names = [r])\n",
    "    interrater_df = interrater_df.merge(raterX, left_index=True, right_index = True)\n",
    "\n",
    "interrater_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total number of \"True\" labels for each annotator\n",
    "print(\"Total Irony labels for true labels: \", str(interrater_df['True_label'].sum()).rjust(9))\n",
    "for r in rater_list:\n",
    "    print(\"Total Irony labels for \"+r+\" annotator: \", str(interrater_df[r].sum()).rjust(10-len(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find ratio that each annotator correctly annotated the tweets\n",
    "\n",
    "coincidence = list()\n",
    "for i in rater_list: # We iterate through all the annotators\n",
    "    x = interrater_df['True_label'][interrater_df['True_label'] == 1] # Transform labels to TRUE/FALSE\n",
    "    y = interrater_df[i][interrater_df[i] == 1] # Transform labels to TRUE/FALSE\n",
    "    ratio = sum(np.bitwise_and(x, y)) / len(x) # Sum all coincidences and divide by the length\n",
    "    coincidence.append(ratio)\n",
    "    print(\"Coincidence ratio for \"+i+ \"{:.1%}\".format(ratio).rjust(15-len(i)))\n",
    "\n",
    "# Print the average coincidence rating for the team\n",
    "print(sum(coincidence)/len(coincidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to compute when all annotators agree on labeling a tweet \"ironic\" or \"not ironic\"\n",
    "interrater_df['agree'] =  ((interrater_df['Dee'] + interrater_df['Sanna'] + interrater_df['Gino'] + \\\n",
    "                            interrater_df['August'] + interrater_df['Moneeca']) == 0) |\\\n",
    "                            (interrater_df['Dee'] + interrater_df['Sanna'] + interrater_df['Gino'] + \\\n",
    "                            interrater_df['August'] + interrater_df['Moneeca'] == 5)\n",
    "\n",
    "agreed = interrater_df['agree'].sum()\n",
    "a_0 = agreed/len(interrater_df)\n",
    "print(f\"All annotators agreed in {agreed} number of observations ({(a_0):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust for chance, assumption that there is a uniform distribution where p = 0.5 to choose 0 or 1.\n",
    "p = 0.5\n",
    "a_c = (p)**len(rater_list)\n",
    "a_adj = (a_0 - a_c)/(1-a_c)\n",
    "\n",
    "print(f'{a_adj:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore our interagreement annotations to find our limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show where annotators do not agree\n",
    "interrater_df[interrater_df['agree'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phenomena that caused the biggest problems for inter-annotator agreement\n",
    "\n",
    "* Lack of context to give understanding of the text. For example, some seem to reference a photo\n",
    "* Some texts refer to specific pop culture or niche subject matter that was not understood or known by the annotators. For example, reference to sports, people, or twitter related subject matter.\n",
    "* Perspective biases of the annotators based on gender, culture, personal background, previous experience or work.\n",
    "* Emojis in the text do not clarify the understanding of the text itself.\n",
    "* Poor grammer/ sentense structure/ spelling, etc making it difficult to understand the text. **Schizophrenic twits**.\n",
    "\n",
    "In addition, we noticed inconsistencies with our own annotation choices (intra-annotations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying and Predicting with Validation Data: Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irony Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), analyzer = 'word',tokenizer=tokenise)), #vectorises\n",
    "                     ('tfidf', TfidfTransformer()), #reduces common word weighting\n",
    "                     ('clf', SGDClassifier(loss='log',shuffle=False))]) #classifier \n",
    "\n",
    "# Run the Irony Training data through the pipeline to create a classifier\n",
    "bob = text_clf.fit(irony_train_df['tweet'], irony_train_df['label'])\n",
    "\n",
    "# Use the classifier to predict the Validation Data\n",
    "bob_predicted = bob.predict(irony_val_df['tweet'])\n",
    "np.mean(bob_predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first ran the code below without comments to test for the most parameter options. Once we found the optimal parameters, we commented out the code pertaining to the best parameters that happen to be the default setting. This way, the notebook would not take too long to run, since we have already obtained the optimal parameter options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), # vectorises\n",
    "                     ('tfidf', TfidfTransformer()), # reduces common word weighting\n",
    "                     ('clf', SGDClassifier())]) # classifies\n",
    "\n",
    "# Parameters to test for best performance\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], # default = (1,1)\n",
    "              #'vect__lowercase': (True, False), # default = True\n",
    "              #'vect__analyzer': ('word','char'), # default = 'word'\n",
    "              'vect__tokenizer': (tokenise, None),\n",
    "              #'tfidf__use_idf': (True, False), # default = True\n",
    "              #'clf__alpha': (1e-4, 1e-7),\n",
    "              'clf__loss': ('log','hinge'),\n",
    "              'clf__shuffle': (False, False) # shuffle = False removes the random element for reproducibility \n",
    "}\n",
    "\n",
    "# GridSearch Classifier\n",
    "irony_sgd_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "\n",
    "# Run Classifer on the Training Data Set\n",
    "irony_sgd_clf = irony_sgd_clf.fit(irony_train_df['tweet'], irony_train_df['label'])\n",
    "\n",
    "# Print GridSearchCV Best Score\n",
    "print(irony_sgd_clf.best_score_)\n",
    "\n",
    "# Print Parameters that were the best\n",
    "for i in irony_sgd_clf.best_params_: print(i,irony_sgd_clf.best_params_[i])\n",
    "\n",
    "# Run the Classifier on the Validation data to Predict\n",
    "irnoy_sgd_predicted = irony_sgd_clf.predict(irony_val_df['tweet'])\n",
    "np.mean(irnoy_sgd_predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), # vectorises\n",
    "                     ('tfidf', TfidfTransformer()), # reduces common word weighting\n",
    "                     ('clf', MultinomialNB())]) #classifies\n",
    "\n",
    "# Parameters to test for best performance\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], # default = (1,1)\n",
    "              #'vect__lowercase': (True, False), # default = True\n",
    "              #'vect__analyzer': ('word','char'), # default = 'word'\n",
    "              'vect__tokenizer': (tokenise, None),\n",
    "              'tfidf__use_idf': (True, False), # default = True\n",
    "              'clf__alpha': (1.0, 2.0) # default = 1.0\n",
    "}\n",
    "\n",
    "# GridSearch Classifier\n",
    "irony_mnb_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "\n",
    "# Run Classifer on the Training Data Set\n",
    "irony_mnb_clf = irony_mnb_clf.fit(irony_train_df['tweet'], irony_train_df['label'])\n",
    "\n",
    "# Print GridSearchCV Best Score\n",
    "print(irony_mnb_clf.best_score_)\n",
    "\n",
    "# Print Parameters that were the best\n",
    "for i in irony_mnb_clf.best_params_: print(i,irony_mnb_clf.best_params_[i])\n",
    "\n",
    "# Run the Classifier on the Validation data to Predict\n",
    "irony_mnb_predicted = irony_mnb_clf.predict(irony_val_df['tweet'])\n",
    "np.mean(irony_mnb_predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), # vectorises\n",
    "                     ('tfidf', TfidfTransformer()), # reduces common word weighting\n",
    "                     ('clf', RandomForestClassifier())]) #classifies\n",
    "\n",
    "# Parameters to test for best performance\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], # default = (1,1)\n",
    "              #'vect__lowercase': (True, False),  # default = True\n",
    "              #'vect__analyzer': ('word','char'), # default = 'word'\n",
    "              'vect__tokenizer': (tokenise, None),\n",
    "              'tfidf__use_idf': (True, False), # default = True\n",
    "              #'clf__max_depth': (5,None,), # default = None\n",
    "              'clf__random_state': (0,None) # default = None\n",
    "}\n",
    "\n",
    "# GridSearch Classifier\n",
    "irony_rfc_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "\n",
    "# Run Classifer on the Training Data Set\n",
    "irony_rfc_clf = irony_rfc_clf.fit(irony_train_df['tweet'], irony_train_df['label'])\n",
    "\n",
    "# Print GridSearchCV Best Score\n",
    "print(irony_rfc_clf.best_score_)\n",
    "\n",
    "# Print Parameters that were the best\n",
    "for i in irony_rfc_clf.best_params_: print(i,irony_rfc_clf.best_params_[i])\n",
    "\n",
    "# Run the Classifier on the Validation data to Predict\n",
    "irnoy_rfc_predicted = irony_rfc_clf.predict(irony_val_df['tweet'])\n",
    "np.mean(irnoy_rfc_predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(irony_val_df['label'], irnoy_sgd_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Overall Model Accuracy: {accuracy_score(irony_val_df[\"label\"], irnoy_sgd_predicted):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix for the SGD classifier model\n",
    "plot_confusion_matrix(irony_sgd_clf, irony_val_df['tweet'], irony_val_df['label'],\n",
    "                                 display_labels=[1, 0],\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize='true')\n",
    "\n",
    "#plt.savefig('../reports/figures/Irony_validation_confusion_matrix.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), # vectorises\n",
    "                     ('tfidf', TfidfTransformer()), # reduces common word weighting\n",
    "                     ('clf', SGDClassifier())]) # classifies\n",
    "\n",
    "# Parameters to test for best performance\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], # default = (1,1)\n",
    "              #'vect__lowercase': (True, False),  # default = True\n",
    "              #'vect__analyzer': ('word','char'), # default = 'word'\n",
    "              'vect__tokenizer': (tokenise, None),\n",
    "              #'tfidf__use_idf': (True, False), # default = True\n",
    "              #'clf__alpha': (1e-4, 1e-7), # default 1e-4\n",
    "              #'clf__loss': ('log','hinge'), # default = 'hinge'\n",
    "              'clf__shuffle': (True, False) # shuffle = False removes the random element for reproducibility \n",
    "}\n",
    "\n",
    "# GridSearch Classifier\n",
    "emoji_sgd_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "\n",
    "# Run Classifer on the Training Data Set\n",
    "emoji_sgd_clf = emoji_sgd_clf.fit(emoji_train_df['tweet'], emoji_train_df['label'])\n",
    "\n",
    "# Print GridSearchCV Best Score\n",
    "print(emoji_sgd_clf.best_score_)\n",
    "\n",
    "# Print Parameters that were the best\n",
    "for i in emoji_sgd_clf.best_params_: print(i,emoji_sgd_clf.best_params_[i])\n",
    "\n",
    "# Run the Classifier on the Validation data to Predict\n",
    "emoji_sgd_predicted = emoji_sgd_clf.predict(emoji_val_df['tweet'])\n",
    "np.mean(emoji_sgd_predicted == emoji_val_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), # vectorises\n",
    "                     ('tfidf', TfidfTransformer()), # reduces common word weighting\n",
    "                     ('clf', MultinomialNB())]) #classifies\n",
    "\n",
    "# Parameters to test for best performance\n",
    "parameters = {#'vect__ngram_range': [(1, 1), (1, 2)], # default = (1,1)\n",
    "              #'vect__lowercase': (True, False),  # default = True\n",
    "              #'vect__analyzer': ('word','char'), # default = 'word'\n",
    "              'vect__tokenizer': (tokenise, None), # default = None\n",
    "              #'tfidf__use_idf': (True, False), # defaultl = True\n",
    "              #'clf__alpha': (1.0, 2.0)\n",
    "}\n",
    "\n",
    "# GridSearch Classifier\n",
    "emoji_mnb_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "\n",
    "# Run Classifer on the Training Data Set\n",
    "emoji_mnb_clf = emoji_mnb_clf.fit(emoji_train_df['tweet'], emoji_train_df['label'])\n",
    "\n",
    "# Print GridSearchCV Best Score\n",
    "print(emoji_mnb_clf.best_score_)\n",
    "\n",
    "# Print Parameters that were the best\n",
    "for i in emoji_mnb_clf.best_params_: print(i,emoji_mnb_clf.best_params_[i])\n",
    "\n",
    "# Run the Classifier on the Validation data to Predict\n",
    "emoji_mnb_predicted = emoji_mnb_clf.predict(emoji_val_df['tweet'])\n",
    "np.mean(emoji_mnb_predicted == emoji_val_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RandomForestClassifier uses a huge amount of time, even when limiting the parameter options. The best rating was was less than the accuracy of the SGD Classifier. Therefore the code for the RandomForestClassifier is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), # vectorises\n",
    "                     ('tfidf', TfidfTransformer()), # reduces common word weighting\n",
    "                     ('clf', RandomForestClassifier())]) #classifies\n",
    "\n",
    "# Parameters to test for best performance\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], # default = (1,1)\n",
    "              #'vect__lowercase': (True, False),  # default = True\n",
    "              #'vect__analyzer': ('word','char'), # default = 'word'\n",
    "              'vect__tokenizer': (tokenise, None),\n",
    "              #'tfidf__use_idf': (True, False), # default = True\n",
    "              #'clf__max_depth': (5,None,), # default = None\n",
    "              #'clf__random_state': (0,None) # default = None\n",
    "}\n",
    "\n",
    "# GridSearch Classifier\n",
    "#emoji_rfc_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "\n",
    "# Run Classifer on the Training Data Set\n",
    "#emoji_rfc_clf = emoji_rfc_clf.fit(emoji_df['tweet'], emoji_df['label'])\n",
    "\n",
    "# Print GridSearchCV Best Score\n",
    "#print(emoji_rfc_clf.best_score_)\n",
    "\n",
    "# Print Parameters that were the best\n",
    "#for i in emoji_rfc_clf.best_params_: print(i,emoji_rfc_clf.best_params_[i])\n",
    "\n",
    "# Predict Based on Classifier\n",
    "#emoji_rfc_predicted = emoji_rfc_clf.predict(emoji_val_df['tweet'])\n",
    "#np.mean(emoji_rfc_predicted == emoji_val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), analyzer = 'word',tokenizer=tokenise)), #vectorises\n",
    "#                     ('tfidf', TfidfTransformer()), # reduces common word weighting\n",
    "#                     ('clf', RandomForestClassifier())]) #classifier \n",
    "\n",
    "#emoji_rfc = text_clf.fit(emoji_df['tweet'], emoji_df['label'])\n",
    "\n",
    "#emoji_rfc_predicted = emoji_rfc.predict(emoji_val_df['tweet'])\n",
    "#np.mean(emoji_rfc_predicted == emoji_val_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(emoji_val_df['label'], emoji_sgd_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Overall Model Accuracy: {accuracy_score(emoji_val_df[\"label\"], emoji_sgd_predicted):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_val_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix for the SGD classifier model\n",
    "fig, ax = plt.subplots(figsize=(17,17))\n",
    "\n",
    "plot_confusion_matrix(emoji_sgd_clf, emoji_val_df['tweet'], emoji_val_df['label'],\n",
    "                                 display_labels=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize='true',\n",
    "                                 ax=ax)\n",
    "\n",
    "#plt.savefig('../reports/figures/Emoji_validation_confusion_matrix.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Using Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irony Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Irony test data into dataframes\n",
    "irony_test = token_data(IRONY_RAW_PATH + TEST, IRONY_INTERIM_PATH + TEST_INTERIM)\n",
    "\n",
    "temp1 = pd.read_csv(IRONY_INTERIM_PATH + TEST_INTERIM, delimiter=\"\\n\", names=['tweet'])\n",
    "temp2 = pd.read_csv(IRONY_RAW_PATH + TEST_LABELS, names=['label'])\n",
    "irony_test_df = temp1.merge(temp2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Classifier on the Test data to Predict\n",
    "irony_test_sgd_predicted = irony_sgd_clf.predict(irony_test_df['tweet'])\n",
    "np.mean(irony_test_sgd_predicted == irony_test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(irony_test_df['label'], irony_test_sgd_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Overall Model Accuracy: {accuracy_score(irony_test_df[\"label\"], irony_test_sgd_predicted):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix for the SGD classifier model\n",
    "fig, ax = plt.subplots()\n",
    "plot_confusion_matrix(irony_sgd_clf, irony_test_df['tweet'], irony_test_df['label'],\n",
    "                                 display_labels=[0,1],\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize='true',\n",
    "                                 ax=ax)\n",
    "\n",
    "#plt.savefig('../reports/figures/Irony_Test_confusion_matrix.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Emoji test data into dataframes\n",
    "emoji_test = token_data(EMOJI_RAW_PATH + TEST, EMOJI_INTERIM_PATH + TEST_INTERIM)\n",
    "\n",
    "temp1 = pd.read_csv(EMOJI_INTERIM_PATH + TEST_INTERIM, delimiter=\"\\n\", names=['tweet'])\n",
    "temp2 = pd.read_csv(EMOJI_RAW_PATH + TEST_LABELS, names=['label'])\n",
    "emoji_test_df = temp1.merge(temp2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Classifier on the Test data to Predict\n",
    "emoji_test_sgd_predicted = emoji_sgd_clf.predict(emoji_test_df['tweet'])\n",
    "np.mean(emoji_test_sgd_predicted == emoji_test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(emoji_test_df['label'], emoji_test_sgd_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Overall Model Accuracy: {accuracy_score(emoji_test_df[\"label\"], emoji_test_sgd_predicted):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix for the SGD classifier model\n",
    "fig, ax = plt.subplots(figsize=(17,17))\n",
    "\n",
    "plot_confusion_matrix(emoji_sgd_clf, emoji_test_df['tweet'], emoji_test_df['label'],\n",
    "                                 display_labels=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize='true',\n",
    "                                 ax=ax)\n",
    "\n",
    "#plt.savefig('../reports/figures/Emoji_test_confusion_matrix.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
