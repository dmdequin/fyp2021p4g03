{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Year Project\n",
    "## Project 4 - Natural Language Processing\n",
    "### Professor - Christian Hardmeier\n",
    "\n",
    "This notebook contains all of the code developed for project 4. We will be using a data set of tweets to perform machine learning for binary and multiclass classification.\n",
    "\n",
    "For **binary** classification, we evaluate tweets based on *'ironic'* or *'not ironic'*. [Learn More](https://www.aclweb.org/anthology/S18-1005.pdf)\n",
    "<br>\n",
    "For **multiclass** classification, we evaluate predict which emojis are used based on the text data. [Learn More](https://www.aclweb.org/anthology/S18-1003.pdf)\n",
    "\n",
    "Group 3:<br>\n",
    "Crisanna Cornish (ccor@itu.dk)<br>\n",
    "Danielle Dequin (ddeq@itu.dk)<br>\n",
    "Gino Franco Fazzi (gifa@itu.dk)<br>\n",
    "Moneeca Abru Iftikhar Latif (abml@itu.dk)<br>\n",
    "Carl August Wismer (cwis@itu.dk)\n",
    "\n",
    "Created: 27-04-2021<br>\n",
    "Last Modified: 04-05-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "We use the TweetEval repository, a collection of 7 datasets for different classification tasks based on social media post. The repository can be found here: https://github.com/cardiffnlp/tweeteval.git\n",
    "\n",
    "Each dataset is presented in the same format and with fixed training, validation and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import NgramCounter\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Laplace\n",
    "from nltk.lm import KneserNeyInterpolated\n",
    "from nltk.lm import WittenBellInterpolated\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irony text:\n",
    "IRONY_RAW_PATH = '../datasets/irony/raw/'\n",
    "IRONY_INTERIM_PATH = '../datasets/irony/interim/'\n",
    "\n",
    "# emoji:\n",
    "EMOJI_RAW_PATH = '../datasets/emoji/raw/'\n",
    "EMOJI_INTERIM_PATH = '../datasets/emoji/interim/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = 'test_text.txt'\n",
    "TRAIN = 'train_text.txt'\n",
    "VAL = 'val_text.txt'\n",
    "\n",
    "TEST_LABELS = 'test_labels.txt'\n",
    "TRAIN_LABELS = 'train_labels.txt'\n",
    "VAL_LABELS = 'val_labels.txt'\n",
    "\n",
    "TEST_INTERIM = 'test_seperated.csv'\n",
    "TRAIN_INTERIM = 'train_seperated.csv'\n",
    "VAL_INTERIM = 'val_seperated.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(line):\n",
    "    # Initialise lists\n",
    "    tokens = []\n",
    "    unmatchable = []\n",
    "\n",
    "    # Compile patterns for speedup\n",
    "    token_pat = re.compile(r'\\w+|#+|\\'|@|\\.\\.+|!+|\\?+')\n",
    "    skippable_pat = re.compile(r',|\\|http://t.co/+')  # typically spaces\n",
    "\n",
    "    # As long as there's any material left...\n",
    "    while line:\n",
    "        # Try finding a skippable token delimiter first.\n",
    "        skippable_match = re.search(skippable_pat, line)\n",
    "        if skippable_match and skippable_match.start() == 0:\n",
    "            # If there is one at the beginning of the line, just skip it.\n",
    "            line = line[skippable_match.end():]\n",
    "        else:\n",
    "            # Else try finding a real token.\n",
    "            token_match = re.search(token_pat, line)\n",
    "            #print(token_match)\n",
    "            if token_match and token_match.start() == 0:\n",
    "                #print(line[token_match.start():token_match.end()])\n",
    "                if line[token_match.start():token_match.end()] == '#': #keep hash tags together and seperate\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        else:\n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                elif line[token_match.start():token_match.end()] == '@': # keep @ tags together and seperate\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        \n",
    "                        else: \n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                elif line[token_match.start():token_match.end()] == \"'\": # handle contractions as a single word\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        \n",
    "                        else: \n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                # If there is one at the beginning of the line, tokenise it.\n",
    "                else:\n",
    "                    tokens.append(line[:token_match.end()])\n",
    "                    line = line[token_match.end():]\n",
    "            else:\n",
    "                # Else there is unmatchable material here.\n",
    "                # It ends where a skippable or token match starts, or at the end of the line.\n",
    "                unmatchable_end = len(line)\n",
    "                if skippable_match:\n",
    "                    unmatchable_end = skippable_match.start()\n",
    "                if token_match:\n",
    "                    unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "                # Add it to unmatchable and discard from line.\n",
    "                unmatchable.append(line[:unmatchable_end])\n",
    "                line = line[unmatchable_end:]\n",
    "\n",
    "    final_tokens = []\n",
    "\n",
    "    while len(tokens) > 0:\n",
    "        temp1 = tokens.pop(0)\n",
    "        try:\n",
    "            temp2 = tokens.pop(0)\n",
    "            if temp2[0] == \"'\":\n",
    "                temp1 += temp2\n",
    "                final_tokens.insert(0, temp1)\n",
    "                #print('a', temp1)\n",
    "            else:\n",
    "                final_tokens.insert(0, temp1)\n",
    "                tokens.insert(0, temp2)\n",
    "                #print('b', temp1, temp2)\n",
    "        except:\n",
    "            final_tokens.insert(0, temp1)\n",
    "            #print('d', temp1)\n",
    "        \n",
    "    final_tokens = final_tokens[::-1]\n",
    "\n",
    "    #print(final_tokens)\n",
    "    #print(unmatchable)\n",
    "    return final_tokens\n",
    "\n",
    "def token_data(data, interim, tokenizer=None):\n",
    "    \"\"\"Function to tokenize from raw text file. Takes a reading file path, a writing file path and a\n",
    "    tokenizer argument (None for default tokenizer, 'Compare' for TweetTokenizer).\n",
    "    Writes a file with the tokenize lines and returns a list (lines) of lists (tokens).\"\"\"\n",
    "    if tokenizer == None:\n",
    "        # Open Irony raw data set text and tokenize\n",
    "        f = open(data, \"r\", encoding=\"utf-8\")\n",
    "        token_list = []\n",
    "        for line in f:\n",
    "            token_list.append(tokenise(line))\n",
    "        f.close()\n",
    "    \n",
    "    elif tokenizer == \"compare\":\n",
    "        tknzr = TweetTokenizer()\n",
    "\n",
    "        f = open(data, \"r\", encoding=\"utf-8\")\n",
    "        token_list = []\n",
    "        for line in f:\n",
    "            token_list.append(tknzr.tokenize(line))\n",
    "        f.close()\n",
    "        \n",
    "    # Write the tokenized data to an interim csv file\n",
    "    with open(interim, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(token_list)\n",
    "        \n",
    "    return token_list\n",
    "\n",
    "def token_counter(tokens, n= 10, inverse=False):\n",
    "    plain = [t for l in tokens for t in l]\n",
    "    counter = Counter(plain)\n",
    "    if not inverse:\n",
    "        return counter.most_common(n)\n",
    "    else:\n",
    "        return counter.most_common()[:-n-1:-1]\n",
    "\n",
    "def zipf_law(tokens, ds_name=''):\n",
    "    x = [log(x) for x in range(1, len(tokens)+1)]\n",
    "    y = []\n",
    "\n",
    "    ordered = token_counter(tokens, n=len(tokens))\n",
    "\n",
    "    for tup in ordered:\n",
    "        word, count = tup\n",
    "        y.append(log(count))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y)\n",
    "    ax.set_xlabel('Log(Rank)')\n",
    "    ax.set_ylabel('Log(Freq)')\n",
    "    ax.set_title(f'Zipf Law for {ds_name} dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data: Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Irony Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Irony data set, tokenize, and write to a csv file\n",
    "irony = token_data(IRONY_RAW_PATH + TRAIN, IRONY_INTERIM_PATH+TRAIN_INTERIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seeing',\n",
       " 'ppl',\n",
       " 'walking',\n",
       " 'w',\n",
       " 'crutches',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'really',\n",
       " 'excited',\n",
       " 'for',\n",
       " 'the',\n",
       " 'next',\n",
       " '3',\n",
       " 'weeks',\n",
       " 'of',\n",
       " 'my',\n",
       " 'life']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irony[0] # A taste of the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Emoji Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Emoji raw dadta set, tokenize, and write to an interim csv file\n",
    "emoji = token_data(EMOJI_RAW_PATH + TRAIN, EMOJI_INTERIM_PATH+TRAIN_INTERIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sunday',\n",
       " 'afternoon',\n",
       " 'walking',\n",
       " 'through',\n",
       " 'Venice',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'with',\n",
       " '@user',\n",
       " 'Abbot',\n",
       " 'Kinney',\n",
       " 'Venice']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji[0] # A test of emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare tokeniser’s output \n",
    "With the baseline tokenisation from the socialmedia tokeniser in the NLTK library (nltk.tokenize.TweetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open emoji data, tokenize with NLTK tokenizer, write to interim csv for comparison\n",
    "compare_emoji = token_data(EMOJI_RAW_PATH + TRAIN, EMOJI_INTERIM_PATH+'train_seperated_compared.csv',\"compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['️', '️', '️', '️', '@', 'Toys', '\"', 'R', '\"', 'Us']\n",
      "['Toys', 'R', 'Us']\n"
     ]
    }
   ],
   "source": [
    "print(compare_emoji[3])\n",
    "print(emoji[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open emoji data, tokenize with NLTK tokenizer, write to interim csv for comparison\n",
    "compare_irony = token_data(IRONY_RAW_PATH + TRAIN, IRONY_INTERIM_PATH+'train_seperated_compared.csv',\"compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@user', '@user', 'So', 'is', 'he', 'banded', 'from', 'wearing', 'the', 'clothes', '?', '#Karma']\n",
      "['@user', '@user', 'So', 'is', 'he', 'banded', 'from', 'wearing', 'the', 'clothes', '?', '#Karma']\n"
     ]
    }
   ],
   "source": [
    "print(compare_irony[3])\n",
    "print(irony[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterize Data: Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.read_csv(IRONY_INTERIM_PATH+TRAIN_INTERIM, delimiter=\"\\n\", names=['tweet'])\n",
    "temp2 = pd.read_csv(IRONY_RAW_PATH+TRAIN_LABELS, names=['label'])\n",
    "irony_df = temp1.merge(temp2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irony Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2862 tweets in the Irony data set\n",
      "Ironic: 1445 \n",
      "Non-ironic: 1417\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {irony_df.shape[0]} tweets in the Irony data set\")\n",
    "print('Ironic:', (irony_df[irony_df['label'] == 1]).shape[0], '\\nNon-ironic:', (irony_df[irony_df['label'] == 0]).shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@user', 1731),\n",
       " ('the', 964),\n",
       " ('to', 934),\n",
       " ('a', 757),\n",
       " ('I', 724),\n",
       " ('is', 483),\n",
       " ('and', 470),\n",
       " ('!', 464),\n",
       " ('in', 440),\n",
       " ('of', 426)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter(irony)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dues', 1),\n",
       " ('shorted', 1),\n",
       " ('union', 1),\n",
       " ('#skincare', 1),\n",
       " ('heaven', 1),\n",
       " ('products', 1),\n",
       " ('thot', 1),\n",
       " ('#practicewhatyoupreach', 1),\n",
       " ('cracks', 1),\n",
       " ('managers', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter(irony, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_tokens = [t for l in irony for t in l] # Get tokens for each line of the irony data set\n",
    "irony_one_timers = []\n",
    "irony_mult_timers = []\n",
    "\n",
    "for tup in token_counter(irony, n=len(irony_tokens)):\n",
    "    k, v = tup # Unpack\n",
    "    if v == 1:\n",
    "        irony_one_timers.append(k)\n",
    "    else:\n",
    "        irony_mult_timers.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that occur only once:7250 (70%)\n",
      "Number of words that occur more than once:3050 (30%)\n",
      "Total word count, including repeated words: 40371\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of words that occur only once:\\\n",
    "{len(irony_one_timers)} ({len(irony_one_timers)/(len(irony_one_timers)+len(irony_mult_timers)):.0%})\\n\\\n",
    "Number of words that occur more than once:\\\n",
    "{len(irony_mult_timers)} ({len(irony_mult_timers)/(len(irony_one_timers)+len(irony_mult_timers)):.0%})\\n\\\n",
    "Total word count, including repeated words: {len(irony_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "zipf_law(emoji, 'Emoji');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simple visual inspection, the graph seems to be consistent with the empirical Zipf Law (a semi-straight line crosses the plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{Type}{Token} ratio$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25513363553045504"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counter(irony, n=len(irony_tokens))) / len(irony_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.read_csv(EMOJI_INTERIM_PATH+TRAIN_INTERIM, delimiter=\"\\n\", names=['tweet'])\n",
    "temp2 = pd.read_csv(EMOJI_RAW_PATH+TRAIN_LABELS, names=['label'])\n",
    "emoji_df = temp1.merge(temp2, left_index=True, right_index=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45000 Tweets in the Emoji data set.\n",
      "0:  9204\n",
      "1:  4901\n",
      "2:  4713\n",
      "3:  2043\n",
      "4:  2146\n",
      "5:  2132\n",
      "6:  2078\n",
      "7:  2345\n",
      "8:  1287\n",
      "9:  1391\n",
      "10:  1982\n",
      "11:  946\n",
      "12:  1246\n",
      "13:  980\n",
      "14:  1224\n",
      "15:  934\n",
      "16:  1350\n",
      "17:  1397\n",
      "18:  1510\n",
      "19:  1191\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {emoji_df.shape[0]} Tweets in the Emoji data set.\") #should be 45000\n",
    "for i in range(20):\n",
    "    print(f'{i}: ', (emoji_df[emoji_df['label'] == i].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@user', 12209),\n",
       " ('the', 10346),\n",
       " ('!', 9045),\n",
       " ('to', 7645),\n",
       " ('I', 6535),\n",
       " ('a', 6100),\n",
       " ('my', 5965),\n",
       " ('in', 5688),\n",
       " ('and', 5389),\n",
       " ('with', 5049)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter(emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Five50', 1),\n",
       " ('#happylaborday', 1),\n",
       " ('Nunez', 1),\n",
       " ('#bffweekend', 1),\n",
       " ('Players', 1),\n",
       " ('#rolltide', 1),\n",
       " ('#Alabama', 1),\n",
       " ('chefking1921express', 1),\n",
       " ('nector', 1),\n",
       " ('thedabberchick', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter(emoji, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_tokens = [t for l in emoji for t in l] # Get tokens for each line of the emoji data set\n",
    "emoji_one_timers = []\n",
    "emoji_mult_timers = []\n",
    "\n",
    "for tup in token_counter(emoji, n=len(emoji_tokens)):\n",
    "    k, v = tup # Unpack\n",
    "    if v == 1:\n",
    "        emoji_one_timers.append(k)\n",
    "    else:\n",
    "        emoji_mult_timers.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that occur only once:49420 (69%)\n",
      "Number of words that occur more than once:21953 (31%)\n",
      "Total word count, including repeated words: 511305\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of words that occur only once:\\\n",
    "{len(emoji_one_timers)} ({len(emoji_one_timers)/(len(emoji_one_timers)+len(emoji_mult_timers)):.0%})\\n\\\n",
    "Number of words that occur more than once:\\\n",
    "{len(emoji_mult_timers)} ({len(emoji_mult_timers)/(len(emoji_one_timers)+len(emoji_mult_timers)):.0%})\\n\\\n",
    "Total word count, including repeated words: {len(emoji_tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "zipf_law(irony, 'Irony');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{Type}{Token} ratio$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1395898729721008"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counter(emoji, n=len(emoji_tokens))) / len(emoji_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Irony"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelyhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm = MLE(2) # Maximum likelyhood estimator or order 2\n",
    "len(lm.vocab) # Initializes an empty vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10303"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(train, vocab) # which is filled with model data\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 10303 items>\n"
     ]
    }
   ],
   "source": [
    "print(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 89328 ngrams>\n"
     ]
    }
   ],
   "source": [
    "lm.vocab.lookup(irony[0])\n",
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(\"<UNK>\") == lm.score(\"aliens\") # The token 'aliens' is not in our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016422605488664713"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(\"a\") # returns the relative frequency of 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.92817315721575"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.logscore(\"a\") # This method avoids underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000954550385074303"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03755287992190042"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(\"@user\") #most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14646464646464646"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the validation set\n",
    "irony_val = token_data(IRONY_RAW_PATH + VAL, IRONY_INTERIM_PATH+VAL_INTERIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10303"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm2 = Laplace(1)\n",
    "len(lm2.vocab) # Initializes an empty vocab\n",
    "lm2.fit(train, vocab) # which is filled with model data\n",
    "len(lm2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 10303 items>\n"
     ]
    }
   ],
   "source": [
    "print(lm2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.score(\"<UNK>\") == lm.score(\"aliens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030710308876201284"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.score(\"@user\") #most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028568707742119798"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.333345004312507"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10321.356737145781"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10303"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm3 = KneserNeyInterpolated(1)\n",
    "print(len(lm3.vocab)) # Initializes an empty vocab\n",
    "lm3.fit(train, vocab) # which is filled with model data\n",
    "len(lm3.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.score(\"<UNK>\") == lm.score(\"aliens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.705910899737941e-05"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.score(\"@user\") #most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1459661646063665"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.802139172913105"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14284.264894756081"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WittenBellInterpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10303\n"
     ]
    }
   ],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm4 = WittenBellInterpolated(1)\n",
    "print(len(lm4.vocab)) # Initializes an empty vocab\n",
    "lm4.fit(train, vocab) # which is filled with model data\n",
    "print(len(lm4.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm4.score(\"<UNK>\") == lm.score(\"aliens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03755287992190042"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm4.score(\"@user\") #most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0939777986008466"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm4.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm4.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm4.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bit of fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ironic = irony_df[irony_df['label'] == 1]['tweet'].reset_index().drop('index', axis=1)\n",
    "ironic_list = [t.split(',') for t in ironic['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, ironic_list)\n",
    "lm_ironic = Laplace(1)\n",
    "lm_ironic.fit(train, vocab)\n",
    "lm_ironic.vocab;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm_ironic.generate(12, random_seed=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interrater Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True_label</th>\n",
       "      <th>Dee</th>\n",
       "      <th>Sanna</th>\n",
       "      <th>Gino</th>\n",
       "      <th>August</th>\n",
       "      <th>Moneeca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   True_label  Dee  Sanna  Gino  August  Moneeca\n",
       "0           1    1      1     1       1        1\n",
       "1           1    0      0     0       0        0\n",
       "2           1    0      0     0       0        0\n",
       "3           0    0      0     1       1        1\n",
       "4           1    1      0     0       1        1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rater_list = ['Dee', 'Sanna', 'Gino', 'August', 'Moneeca']\n",
    "\n",
    "interrater_df = pd.read_csv('../datasets/iaa-sets/irony/iaa_labels.txt', names = ['True_label'])\n",
    "\n",
    "for r in rater_list:\n",
    "    raterX = pd.read_csv('../datasets/iaa-sets/irony/'+r+'Annotation.txt', names = [r])\n",
    "    interrater_df = interrater_df.merge(raterX, left_index=True, right_index = True)\n",
    "\n",
    "interrater_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Irony labels for true labels:         55\n",
      "Total Irony labels for Dee annotator:       44\n",
      "Total Irony labels for Sanna annotator:     25\n",
      "Total Irony labels for Gino annotator:      51\n",
      "Total Irony labels for August annotator:    31\n",
      "Total Irony labels for Moneeca annotator:   46\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Irony labels for true labels: \", str(interrater_df['True_label'].sum()).rjust(9))\n",
    "for r in rater_list:\n",
    "    print(\"Total Irony labels for \"+r+\" annotator: \", str(interrater_df[r].sum()).rjust(10-len(r)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coincidence ratio for Dee       63.6%\n",
      "Coincidence ratio for Sanna     38.2%\n",
      "Coincidence ratio for Gino      60.0%\n",
      "Coincidence ratio for August    50.9%\n",
      "Coincidence ratio for Moneeca   60.0%\n"
     ]
    }
   ],
   "source": [
    "for i in rater_list: # We iterate through all the annotators\n",
    "    x = interrater_df['True_label'][interrater_df['True_label'] == 1] # Transform labels to TRUE/FALSE\n",
    "    y = interrater_df[i][interrater_df[i] == 1] # Transform labels to TRUE/FALSE\n",
    "    ratio = sum(np.bitwise_and(x, y)) / len(x) # Sum all coincidences and divide by the length\n",
    "    print(\"Coincidence ratio for \"+i+ \"{:.1%}\".format(ratio).rjust(15-len(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All annotators agreed in 48 number of observations (40.0%)\n"
     ]
    }
   ],
   "source": [
    "interrater_df['agree'] =  ((interrater_df['Dee'] + interrater_df['Sanna'] + interrater_df['Gino'] + \\\n",
    "                           interrater_df['August'] + interrater_df['Moneeca']) == 0) |\\\n",
    "(interrater_df['Dee'] + interrater_df['Sanna'] + interrater_df['Gino'] + \\\n",
    "                           interrater_df['August'] + interrater_df['Moneeca'] == 5)\n",
    "\n",
    "agreed = interrater_df['agree'].sum()\n",
    "print(f\"All annotators agreed in {agreed} number of observations ({agreed/len(interrater_df):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-b8c192f14fd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ma_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrater_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0ma_adj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma_0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ma_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ma_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{a_adj:.1%}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a_0' is not defined"
     ]
    }
   ],
   "source": [
    "## adjust for chance, assumption that there is a uniform distribution where p = 0.5 to choose 0 or 1.\n",
    "p = 0.5\n",
    "a_c = (p)**len(rater_list)\n",
    "\n",
    "a_adj = (a_0 - a_c)/(1-a_c)\n",
    "\n",
    "print(f'{a_adj:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore our interagreement annotations to find our limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interrater_df[interrater_df['agree'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phenomena that caused the biggest problems for inter-annotator agreement\n",
    "\n",
    "* Lack of context to give understanding of the text. For example, some seem to reference a photo\n",
    "* Some texts refer to specific pop culture or niche subject matter that was not understood or known by the annotators. For example, reference to sports, people, or twitter related subject matter.\n",
    "* Perspective biases of the annotators based on gender, culture, personal background, previous experience or work.\n",
    "* Emojis in the text do not clarify the understanding of the text itself.\n",
    "* Poor grammer/ sentense structure/ spelling, etc making it difficult to understand the text. **Schizophrenic twits**.\n",
    "\n",
    "In addition, we noticed inconsistencies with our own annotation choices (intra-annotations).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
