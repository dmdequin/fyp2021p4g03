{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Year Project\n",
    "## Project 4 - Natural Language Processing\n",
    "### Professor - Christian Hardmeier\n",
    "\n",
    "This notebook contains all of the code developed for project 4. We will be using a data set of tweets to perform machine learning for binary and multiclass classification.\n",
    "\n",
    "For **binary** classification, we evaluate tweets based on *'ironic'* or *'not ironic'*. [Learn More](https://www.aclweb.org/anthology/S18-1005.pdf)\n",
    "<br>\n",
    "For **multiclass** classification, we evaluate predict which emojis are used based on the text data. [Learn More](https://www.aclweb.org/anthology/S18-1003.pdf)\n",
    "\n",
    "Group 3:<br>\n",
    "Crisanna Cornish (ccor@itu.dk)<br>\n",
    "Danielle Dequin (ddeq@itu.dk)<br>\n",
    "Gino Franco Fazzi (gifa@itu.dk)<br>\n",
    "Moneeca Abru Iftikhar Latif (abml@itu.dk)<br>\n",
    "Carl August Wismer (cwis@itu.dk)\n",
    "\n",
    "Created: 27-04-2021<br>\n",
    "Last Modified: 14-05-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "We use the TweetEval repository, a collection of 7 datasets for different classification tasks based on social media post. The repository can be found here: https://github.com/cardiffnlp/tweeteval.git\n",
    "\n",
    "Each dataset is presented in the same format and with fixed training, validation and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import NgramCounter\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Laplace\n",
    "from nltk.lm import KneserNeyInterpolated\n",
    "from nltk.lm import WittenBellInterpolated\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irony text:\n",
    "IRONY_RAW_PATH = '../datasets/irony/raw/'\n",
    "IRONY_INTERIM_PATH = '../datasets/irony/interim/'\n",
    "\n",
    "# emoji:\n",
    "EMOJI_RAW_PATH = '../datasets/emoji/raw/'\n",
    "EMOJI_INTERIM_PATH = '../datasets/emoji/interim/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = 'test_text.txt'\n",
    "TRAIN = 'train_text.txt'\n",
    "VAL = 'val_text.txt'\n",
    "\n",
    "TEST_LABELS = 'test_labels.txt'\n",
    "TRAIN_LABELS = 'train_labels.txt'\n",
    "VAL_LABELS = 'val_labels.txt'\n",
    "\n",
    "TEST_INTERIM = 'test_seperated.csv'\n",
    "TRAIN_INTERIM = 'train_seperated.csv'\n",
    "VAL_INTERIM = 'val_seperated.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(line):\n",
    "    # Initialise lists\n",
    "    tokens = []\n",
    "    unmatchable = []\n",
    "\n",
    "    # Compile patterns for speedup\n",
    "    token_pat = re.compile(r'\\w+|#+|\\'|@|\\.\\.+|!+|\\?+')\n",
    "    skippable_pat = re.compile(r',|\\|http://t.co/+')  # typically spaces\n",
    "\n",
    "    # As long as there's any material left...\n",
    "    while line:\n",
    "        # Try finding a skippable token delimiter first.\n",
    "        skippable_match = re.search(skippable_pat, line)\n",
    "        if skippable_match and skippable_match.start() == 0:\n",
    "            # If there is one at the beginning of the line, just skip it.\n",
    "            line = line[skippable_match.end():]\n",
    "        else:\n",
    "            # Else try finding a real token.\n",
    "            token_match = re.search(token_pat, line)\n",
    "            #print(token_match)\n",
    "            if token_match and token_match.start() == 0:\n",
    "                #print(line[token_match.start():token_match.end()])\n",
    "                if line[token_match.start():token_match.end()] == '#': #keep hash tags together and seperate\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        else:\n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                elif line[token_match.start():token_match.end()] == '@': # keep @ tags together and seperate\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        \n",
    "                        else: \n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                elif line[token_match.start():token_match.end()] == \"'\": # handle contractions as a single word\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        \n",
    "                        else: \n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                # If there is one at the beginning of the line, tokenise it.\n",
    "                else:\n",
    "                    tokens.append(line[:token_match.end()])\n",
    "                    line = line[token_match.end():]\n",
    "            else:\n",
    "                # Else there is unmatchable material here.\n",
    "                # It ends where a skippable or token match starts, or at the end of the line.\n",
    "                unmatchable_end = len(line)\n",
    "                if skippable_match:\n",
    "                    unmatchable_end = skippable_match.start()\n",
    "                if token_match:\n",
    "                    unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "                # Add it to unmatchable and discard from line.\n",
    "                unmatchable.append(line[:unmatchable_end])\n",
    "                line = line[unmatchable_end:]\n",
    "\n",
    "    final_tokens = []\n",
    "\n",
    "    while len(tokens) > 0:\n",
    "        temp1 = tokens.pop(0)\n",
    "        try:\n",
    "            temp2 = tokens.pop(0)\n",
    "            if temp2[0] == \"'\":\n",
    "                temp1 += temp2\n",
    "                final_tokens.insert(0, temp1)\n",
    "                #print('a', temp1)\n",
    "            else:\n",
    "                final_tokens.insert(0, temp1)\n",
    "                tokens.insert(0, temp2)\n",
    "                #print('b', temp1, temp2)\n",
    "        except:\n",
    "            final_tokens.insert(0, temp1)\n",
    "            #print('d', temp1)\n",
    "        \n",
    "    final_tokens = final_tokens[::-1]\n",
    "\n",
    "    #print(final_tokens)\n",
    "    #print(unmatchable)\n",
    "    return final_tokens\n",
    "\n",
    "def token_data(data, interim, tokenizer=None):\n",
    "    \"\"\"Function to tokenize from raw text file. Takes a reading file path, a writing file path and a\n",
    "    tokenizer argument (None for default tokenizer, 'Compare' for TweetTokenizer).\n",
    "    Writes a file with the tokenize lines and returns a list (lines) of lists (tokens).\"\"\"\n",
    "    if tokenizer == None:\n",
    "        # Open Irony raw data set text and tokenize\n",
    "        f = open(data, \"r\", encoding=\"utf-8\")\n",
    "        token_list = []\n",
    "        for line in f:\n",
    "            token_list.append(tokenise(line))\n",
    "        f.close()\n",
    "    \n",
    "    elif tokenizer == \"compare\":\n",
    "        tknzr = TweetTokenizer()\n",
    "\n",
    "        f = open(data, \"r\", encoding=\"utf-8\")\n",
    "        token_list = []\n",
    "        for line in f:\n",
    "            token_list.append(tknzr.tokenize(line))\n",
    "        f.close()\n",
    "        \n",
    "    # Write the tokenized data to an interim csv file\n",
    "    with open(interim, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(token_list)\n",
    "        \n",
    "    return token_list\n",
    "\n",
    "def token_counter(tokens, n= 10, inverse=False):\n",
    "    plain = [t for l in tokens for t in l]\n",
    "    counter = Counter(plain)\n",
    "    if not inverse:\n",
    "        return counter.most_common(n)\n",
    "    else:\n",
    "        return counter.most_common()[:-n-1:-1]\n",
    "\n",
    "def zipf_law(tokens, ds_name=''):\n",
    "    x = [log(x) for x in range(1, len(tokens)+1)]\n",
    "    y = []\n",
    "\n",
    "    ordered = token_counter(tokens, n=len(tokens))\n",
    "\n",
    "    for tup in ordered:\n",
    "        word, count = tup\n",
    "        y.append(log(count))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y)\n",
    "    ax.set_xlabel('Log(Rank)')\n",
    "    ax.set_ylabel('Log(Freq)')\n",
    "    ax.set_title(f'Zipf Law for {ds_name} dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data: Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Irony Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Irony data set, tokenize, and write to a csv file\n",
    "irony = token_data(IRONY_RAW_PATH + TRAIN, IRONY_INTERIM_PATH+TRAIN_INTERIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "irony[0] # A taste of the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Emoji Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Emoji raw dadta set, tokenize, and write to an interim csv file\n",
    "emoji = token_data(EMOJI_RAW_PATH + TRAIN, EMOJI_INTERIM_PATH+TRAIN_INTERIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji[0] # A test of emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare tokeniser’s output \n",
    "With the baseline tokenisation from the socialmedia tokeniser in the NLTK library (nltk.tokenize.TweetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open emoji data, tokenize with NLTK tokenizer, write to interim csv for comparison\n",
    "compare_emoji = token_data(EMOJI_RAW_PATH + TRAIN, EMOJI_INTERIM_PATH+'train_seperated_compared.csv',\"compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_emoji[3])\n",
    "print(emoji[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open emoji data, tokenize with NLTK tokenizer, write to interim csv for comparison\n",
    "compare_irony = token_data(IRONY_RAW_PATH + TRAIN, IRONY_INTERIM_PATH+'train_seperated_compared.csv',\"compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_irony[3])\n",
    "print(irony[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterize Data: Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.read_csv(IRONY_INTERIM_PATH+TRAIN_INTERIM, delimiter=\"\\n\", names=['tweet'])\n",
    "temp2 = pd.read_csv(IRONY_RAW_PATH+TRAIN_LABELS, names=['label'])\n",
    "irony_df = temp1.merge(temp2, left_index=True, right_index=True)\n",
    "\n",
    "temp1 = pd.read_csv(IRONY_INTERIM_PATH+VAL_INTERIM, delimiter=\"\\n\", names=['tweet'])\n",
    "temp2 = pd.read_csv(IRONY_RAW_PATH+VAL_LABELS, names=['label'])\n",
    "irony_val_df = temp1.merge(temp2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irony Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {irony_df.shape[0]} tweets in the Irony data set\")\n",
    "print('Ironic:', (irony_df[irony_df['label'] == 1]).shape[0], '\\nNon-ironic:', (irony_df[irony_df['label'] == 0]).shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter(irony)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter(irony, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_tokens = [t for l in irony for t in l] # Get tokens for each line of the irony data set\n",
    "irony_one_timers = []\n",
    "irony_mult_timers = []\n",
    "\n",
    "for tup in token_counter(irony, n=len(irony_tokens)):\n",
    "    k, v = tup # Unpack\n",
    "    if v == 1:\n",
    "        irony_one_timers.append(k)\n",
    "    else:\n",
    "        irony_mult_timers.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of words that occur only once:\\\n",
    "{len(irony_one_timers)} ({len(irony_one_timers)/(len(irony_one_timers)+len(irony_mult_timers)):.0%})\\n\\\n",
    "Number of words that occur more than once:\\\n",
    "{len(irony_mult_timers)} ({len(irony_mult_timers)/(len(irony_one_timers)+len(irony_mult_timers)):.0%})\\n\\\n",
    "Total word count, including repeated words: {len(irony_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipf_law(emoji, 'Emoji');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simple visual inspection, the graph seems to be consistent with the empirical Zipf Law (a semi-straight line crosses the plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{Type}{Token} ratio$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of number of types (vocabulary size) to number of tokens\n",
    "(text/corpus size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_counter(irony, n=len(irony_tokens))) / len(irony_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.read_csv(EMOJI_INTERIM_PATH+TRAIN_INTERIM, delimiter=\"\\n\", names=['tweet'])\n",
    "temp2 = pd.read_csv(EMOJI_RAW_PATH+TRAIN_LABELS, names=['label'])\n",
    "emoji_df = temp1.merge(temp2, left_index=True, right_index=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {emoji_df.shape[0]} Tweets in the Emoji data set.\") #should be 45000\n",
    "for i in range(20):\n",
    "    print(f'{i}: ', (emoji_df[emoji_df['label'] == i].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_counter(emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter(emoji, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_tokens = [t for l in emoji for t in l] # Get tokens for each line of the emoji data set\n",
    "emoji_one_timers = []\n",
    "emoji_mult_timers = []\n",
    "\n",
    "for tup in token_counter(emoji, n=len(emoji_tokens)):\n",
    "    k, v = tup # Unpack\n",
    "    if v == 1:\n",
    "        emoji_one_timers.append(k)\n",
    "    else:\n",
    "        emoji_mult_timers.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of words that occur only once:\\\n",
    "{len(emoji_one_timers)} ({len(emoji_one_timers)/(len(emoji_one_timers)+len(emoji_mult_timers)):.0%})\\n\\\n",
    "Number of words that occur more than once:\\\n",
    "{len(emoji_mult_timers)} ({len(emoji_mult_timers)/(len(emoji_one_timers)+len(emoji_mult_timers)):.0%})\\n\\\n",
    "Total word count, including repeated words: {len(emoji_tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipf_law(irony, 'Irony');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simple visual inspection, the graph seems to be consistent with the empirical Zipf Law (a semi-straight line crosses the plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{Type}{Token} ratio$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of number of types (vocabulary size) to number of tokens\n",
    "(text/corpus size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_counter(emoji, n=len(emoji_tokens))) / len(emoji_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the vocabulary on emoji tweets is narrower than the one on irony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Irony"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelyhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm = MLE(2) # Maximum likelyhood estimator or order 2\n",
    "len(lm.vocab) # Initializes an empty vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(train, vocab) # which is filled with model data\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.vocab.lookup(irony[0])\n",
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(\"<UNK>\") == lm.score(\"aliens\") # The token 'aliens' is not in our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(\"a\") # returns the relative frequency of 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.logscore(\"a\") # This method avoids underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(\"@user\") #most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the validation set\n",
    "irony_val = token_data(IRONY_RAW_PATH + VAL, IRONY_INTERIM_PATH+VAL_INTERIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm2 = Laplace(1)\n",
    "len(lm2.vocab) # Initializes an empty vocab\n",
    "lm2.fit(train, vocab) # which is filled with model data\n",
    "len(lm2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.score(\"<UNK>\") == lm.score(\"aliens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.score(\"@user\") #most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm3 = KneserNeyInterpolated(1)\n",
    "print(len(lm3.vocab)) # Initializes an empty vocab\n",
    "lm3.fit(train, vocab) # which is filled with model data\n",
    "len(lm3.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.score(\"<UNK>\") == lm.score(\"aliens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.score(\"@user\") #most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WittenBellInterpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, irony)\n",
    "lm4 = WittenBellInterpolated(1)\n",
    "print(len(lm4.vocab)) # Initializes an empty vocab\n",
    "lm4.fit(train, vocab) # which is filled with model data\n",
    "print(len(lm4.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm4.score(\"<UNK>\") == lm.score(\"aliens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm4.score(\"@user\") #most common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm4.score(\"a\", [\"be\"]) # Chance that 'a' is preceeded by 'be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm4.entropy(irony_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm4.perplexity(irony_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bit of fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ironic = irony_df[irony_df['label'] == 1]['tweet'].reset_index().drop('index', axis=1)\n",
    "ironic_list = [t.split(',') for t in ironic['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, ironic_list)\n",
    "lm_ironic = Laplace(1)\n",
    "lm_ironic.fit(train, vocab)\n",
    "lm_ironic.vocab;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm_ironic.generate(12, random_seed=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interrater Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rater_list = ['Dee', 'Sanna', 'Gino', 'August', 'Moneeca']\n",
    "\n",
    "interrater_df = pd.read_csv('../datasets/iaa-sets/irony/iaa_labels.txt', names = ['True_label'])\n",
    "\n",
    "for r in rater_list:\n",
    "    raterX = pd.read_csv('../datasets/iaa-sets/irony/'+r+'Annotation.txt', names = [r])\n",
    "    interrater_df = interrater_df.merge(raterX, left_index=True, right_index = True)\n",
    "\n",
    "interrater_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Irony labels for true labels: \", str(interrater_df['True_label'].sum()).rjust(9))\n",
    "for r in rater_list:\n",
    "    print(\"Total Irony labels for \"+r+\" annotator: \", str(interrater_df[r].sum()).rjust(10-len(r)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rater_list: # We iterate through all the annotators\n",
    "    x = interrater_df['True_label'][interrater_df['True_label'] == 1] # Transform labels to TRUE/FALSE\n",
    "    y = interrater_df[i][interrater_df[i] == 1] # Transform labels to TRUE/FALSE\n",
    "    ratio = sum(np.bitwise_and(x, y)) / len(x) # Sum all coincidences and divide by the length\n",
    "    print(\"Coincidence ratio for \"+i+ \"{:.1%}\".format(ratio).rjust(15-len(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interrater_df['agree'] =  ((interrater_df['Dee'] + interrater_df['Sanna'] + interrater_df['Gino'] + \\\n",
    "                            interrater_df['August'] + interrater_df['Moneeca']) == 0) |\\\n",
    "                            (interrater_df['Dee'] + interrater_df['Sanna'] + interrater_df['Gino'] + \\\n",
    "                            interrater_df['August'] + interrater_df['Moneeca'] == 5)\n",
    "\n",
    "agreed = interrater_df['agree'].sum()\n",
    "a_0 = agreed/len(interrater_df)\n",
    "print(f\"All annotators agreed in {agreed} number of observations ({(a_0):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adjust for chance, assumption that there is a uniform distribution where p = 0.5 to choose 0 or 1.\n",
    "p = 0.5\n",
    "a_c = (p)**len(rater_list)\n",
    "\n",
    "a_adj = (a_0 - a_c)/(1-a_c)\n",
    "\n",
    "print(f'{a_adj:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore our interagreement annotations to find our limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interrater_df[interrater_df['agree'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phenomena that caused the biggest problems for inter-annotator agreement\n",
    "\n",
    "* Lack of context to give understanding of the text. For example, some seem to reference a photo\n",
    "* Some texts refer to specific pop culture or niche subject matter that was not understood or known by the annotators. For example, reference to sports, people, or twitter related subject matter.\n",
    "* Perspective biases of the annotators based on gender, culture, personal background, previous experience or work.\n",
    "* Emojis in the text do not clarify the understanding of the text itself.\n",
    "* Poor grammer/ sentense structure/ spelling, etc making it difficult to understand the text. **Schizophrenic twits**.\n",
    "\n",
    "In addition, we noticed inconsistencies with our own annotation choices (intra-annotations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_df[\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1), analyzer = 'word',tokenizer=tokenise)), #vectorises\n",
    "                     ('tfidf', TfidfTransformer()), #reduces common word weighting\n",
    "                     ('clf', SGDClassifier(loss='log',shuffle=False))]) #classifier, shuffle=False removes the random element for reproducibility\n",
    "\n",
    "bob = text_clf.fit(irony_df['tweet'], irony_df['label'])\n",
    "\n",
    "predicted = bob.predict(irony_val_df['tweet'])\n",
    "np.mean(predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1), analyzer = 'word')), #vectorises\n",
    "                     ('tfidf', TfidfTransformer()), #reduces common word weighting\n",
    "                     ('clf', SGDClassifier(loss='log',shuffle=False))]) #classifier, shuffle=False removes the random element for reproducibility\n",
    "\n",
    "bob = text_clf.fit(irony_df['tweet'], irony_df['label'])\n",
    "\n",
    "predicted = bob.predict(irony_val_df['tweet'])\n",
    "np.mean(predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1), lowercase=False)), #vectorises\n",
    "                     ('tfidf', TfidfTransformer()), #reduces common word weighting\n",
    "                     ('clf', SGDClassifier(loss='log',shuffle=False))]) #classifier, shuffle=False removes the random element for reproducibility\n",
    "\n",
    "bob = text_clf.fit(irony_df['tweet'], irony_df['label'])\n",
    "\n",
    "predicted = bob.predict(irony_val_df['tweet'])\n",
    "np.mean(predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), lowercase=False)), #vectorises\n",
    "                     ('tfidf', TfidfTransformer()), #reduces common word weighting\n",
    "                     ('clf', SGDClassifier(loss='log',shuffle=False, alpha=0.00000000001))]) #classifier, shuffle=False removes the random element for reproducibility\n",
    "\n",
    "bob = text_clf.fit(irony_df['tweet'], irony_df['label'])\n",
    "\n",
    "predicted = bob.predict(irony_val_df['tweet'])\n",
    "np.mean(predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), lowercase=False)), #vectorises\n",
    "                     ('tfidf', TfidfTransformer()), #reduces common word weighting\n",
    "                     ('clf', SGDClassifier(loss='log',shuffle=False))]) #classifier, shuffle=False removes the random element for reproducibility\n",
    "\n",
    "bob = text_clf.fit(irony_df['tweet'], irony_df['label'])\n",
    "\n",
    "predicted = bob.predict(irony_val_df['tweet'])\n",
    "np.mean(predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-7),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(irony_val_df['tweet'], irony_val_df['label'])\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,3), lowercase=False)), #vectorises\n",
    "                     ('tfidf', TfidfTransformer()), #reduces common word weighting\n",
    "                     ('clf', SGDClassifier(loss='log',shuffle=False))]) #classifier, shuffle=False removes the random element for reproducibility\n",
    "\n",
    "bob = text_clf.fit(irony_df['tweet'], irony_df['label'])\n",
    "\n",
    "predicted = bob.predict(irony_val_df['tweet'])\n",
    "np.mean(predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), lowercase=False)), #vectorises\n",
    "                     ('tfidf', TfidfTransformer()), #reduces common word weighting\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "bob = text_clf.fit(irony_df['tweet'], irony_df['label'])\n",
    "\n",
    "predicted = bob.predict(irony_val_df['tweet'])\n",
    "np.mean(predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), lowercase=False)), #vectorises\n",
    "                     ('tfidf', TfidfTransformer()), #reduces common word weighting\n",
    "                     ('clf', SGDClassifier(loss='hinge',shuffle=False))])\n",
    "\n",
    "bob = text_clf.fit(irony_df['tweet'], irony_df['label'])\n",
    "\n",
    "predicted = bob.predict(irony_val_df['tweet'])\n",
    "np.mean(predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), lowercase=False)), #vectorises\n",
    "                     ('tfidf', TfidfTransformer()), #reduces common word weighting\n",
    "                     ('clf', RandomForestClassifier(max_depth=5, random_state=0))])\n",
    "\n",
    "bob = text_clf.fit(irony_df['tweet'], irony_df['label'])\n",
    "\n",
    "predicted = bob.predict(irony_val_df['tweet'])\n",
    "np.mean(predicted == irony_val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
