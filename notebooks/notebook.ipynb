{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Year Project\n",
    "## Project 4 - Natural Language Processing\n",
    "### Professor - Christian Hardmeier\n",
    "\n",
    "This notebook contains all of the code developed for project 4. We will be using a data set of tweets to perform machine learning for binary and multiclass classification.\n",
    "\n",
    "For **binary** classification, we evaluate tweets based on *'ironic'* or *'not ironic'*.<br>\n",
    "For **multiclass** classification, we evaluate predict which emojis are used based on the text data.\n",
    "\n",
    "Group 3:<br>\n",
    "Crisanna Cornish (ccor@itu.dk)<br>\n",
    "Danielle Dequin (ddeq@itu.dk)<br>\n",
    "Gino Franco Fazzi (gifa@itu.dk)<br>\n",
    "Moneeca Abru Iftikhar Latif (abml@itu.dk)<br>\n",
    "Carl August Wismer (cwis@itu.dk)\n",
    "\n",
    "Created: 27-04-2021<br>\n",
    "Last Modified: 04-05-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "We use the TweetEval repository, a collection of 7 datasets for different classification tasks based on social media post. The repository can be found here: https://github.com/cardiffnlp/tweeteval.git\n",
    "\n",
    "Each dataset is presented in the same format and with fixed training, validation and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irony text:\n",
    "IRONY_RAW_PATH = '../datasets/irony/raw/'\n",
    "IRONY_INTERIM_PATH = '../datasets/irony/interim/'\n",
    "\n",
    "# emoji:\n",
    "EMOJI_RAW_PATH = '../datasets/emoji/raw/'\n",
    "EMOJI_INTERIM_PATH = '../datasets/emoji/interim/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = 'test_text.txt'\n",
    "TRAIN = 'train_text.txt'\n",
    "VAL = 'val_text.txt'\n",
    "\n",
    "TEST_LABELS = 'test_labels.txt'\n",
    "TRAIN_LABELS = 'train_labels.txt'\n",
    "VAL_LABELS = 'val_labels.txt'\n",
    "\n",
    "TEST_INTERIM = 'test_seperated.csv'\n",
    "TRAIN_INTERIM = 'train_seperated.csv'\n",
    "VAL_INTERIM = 'val_seperated.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(line):\n",
    "    # Initialise lists\n",
    "    tokens = []\n",
    "    unmatchable = []\n",
    "\n",
    "    # Compile patterns for speedup\n",
    "    token_pat = re.compile(r'\\w+|#+|\\'|@|\\.\\.+|!+|\\?+')\n",
    "    skippable_pat = re.compile(r',|\\|http://t.co/+')  # typically spaces\n",
    "\n",
    "    # As long as there's any material left...\n",
    "    while line:\n",
    "        # Try finding a skippable token delimiter first.\n",
    "        skippable_match = re.search(skippable_pat, line)\n",
    "        if skippable_match and skippable_match.start() == 0:\n",
    "            # If there is one at the beginning of the line, just skip it.\n",
    "            line = line[skippable_match.end():]\n",
    "        else:\n",
    "            # Else try finding a real token.\n",
    "            token_match = re.search(token_pat, line)\n",
    "            #print(token_match)\n",
    "            if token_match and token_match.start() == 0:\n",
    "                #print(line[token_match.start():token_match.end()])\n",
    "                if line[token_match.start():token_match.end()] == '#': #keep hash tags together and seperate\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        else:\n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                elif line[token_match.start():token_match.end()] == '@': # keep @ tags together and seperate\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        \n",
    "                        else: \n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                elif line[token_match.start():token_match.end()] == \"'\": # handle contractions as a single word\n",
    "                    try:\n",
    "                        token_match2 = re.search(token_pat, line[1:])\n",
    "                        if ' ' in line[token_match2.start():token_match2.end()]:\n",
    "                            line = line[token_match.end():]\n",
    "                        \n",
    "                        else: \n",
    "                            tokens.append(line[:token_match2.end()+1])\n",
    "                            line = line[token_match2.end()+1:]\n",
    "                    except:\n",
    "                        line = line[token_match.end():]\n",
    "\n",
    "                # If there is one at the beginning of the line, tokenise it.\n",
    "                else:\n",
    "                    tokens.append(line[:token_match.end()])\n",
    "                    line = line[token_match.end():]\n",
    "            else:\n",
    "                # Else there is unmatchable material here.\n",
    "                # It ends where a skippable or token match starts, or at the end of the line.\n",
    "                unmatchable_end = len(line)\n",
    "                if skippable_match:\n",
    "                    unmatchable_end = skippable_match.start()\n",
    "                if token_match:\n",
    "                    unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "                # Add it to unmatchable and discard from line.\n",
    "                unmatchable.append(line[:unmatchable_end])\n",
    "                line = line[unmatchable_end:]\n",
    "\n",
    "    final_tokens = []\n",
    "\n",
    "    while len(tokens) > 0:\n",
    "        temp1 = tokens.pop(0)\n",
    "        try:\n",
    "            temp2 = tokens.pop(0)\n",
    "            if temp2[0] == \"'\":\n",
    "                temp1 += temp2\n",
    "                final_tokens.insert(0, temp1)\n",
    "                #print('a', temp1)\n",
    "            else:\n",
    "                final_tokens.insert(0, temp1)\n",
    "                tokens.insert(0, temp2)\n",
    "                #print('b', temp1, temp2)\n",
    "        except:\n",
    "            final_tokens.insert(0, temp1)\n",
    "            #print('d', temp1)\n",
    "        \n",
    "    final_tokens = final_tokens[::-1]\n",
    "\n",
    "    #print(final_tokens)\n",
    "    #print(unmatchable)\n",
    "    return final_tokens\n",
    "\n",
    "def token_data(data, interim, tokenizer=None):\n",
    "    \"\"\"Function to tokenize from raw text file. Takes a reading file path, a writing file path and a\n",
    "    tokenizer argument (None for default tokenizer, 'Compare' for TweetTokenizer).\n",
    "    Writes a file with the tokenize lines and returns a list (lines) of lists (tokens).\"\"\"\n",
    "    if tokenizer == None:\n",
    "        # Open Irony raw data set text and tokenize\n",
    "        f = open(data, \"r\", encoding=\"utf-8\")\n",
    "        token_list = []\n",
    "        for line in f:\n",
    "            token_list.append(tokenise(line))\n",
    "        f.close()\n",
    "    \n",
    "    elif tokenizer == \"compare\":\n",
    "        tknzr = TweetTokenizer()\n",
    "\n",
    "        f = open(data, \"r\", encoding=\"utf-8\")\n",
    "        token_list = []\n",
    "        for line in f:\n",
    "            token_list.append(tknzr.tokenize(line))\n",
    "        f.close()\n",
    "        \n",
    "    # Write the tokenized data to an interim csv file\n",
    "    with open(interim, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(token_list)\n",
    "        \n",
    "    return token_list\n",
    "\n",
    "def token_counter(tokens, n= 10, inverse=False):\n",
    "    plain = [t for l in tokens for t in l]\n",
    "    counter = Counter(plain)\n",
    "    if not inverse:\n",
    "        return counter.most_common(n)\n",
    "    else:\n",
    "        return counter.most_common()[:-n-1:-1]\n",
    "\n",
    "def zipf_law(tokens, ds_name=''):\n",
    "    x = [log(x) for x in range(1, len(tokens)+1)]\n",
    "    y = []\n",
    "\n",
    "    ordered = token_counter(tokens, n=len(tokens))\n",
    "\n",
    "    for tup in ordered:\n",
    "        word, count = tup\n",
    "        y.append(log(count))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y)\n",
    "    ax.set_xlabel('Log(Rank)')\n",
    "    ax.set_ylabel('Log(Freq)')\n",
    "    ax.set_title(f'Zipf Law for {ds_name} dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data: Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Irony Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Irony data set, tokenize, and write to a csv file\n",
    "irony = token_data(IRONY_RAW_PATH + TRAIN, IRONY_INTERIM_PATH+TRAIN_INTERIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seeing',\n",
       " 'ppl',\n",
       " 'walking',\n",
       " 'w',\n",
       " 'crutches',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'really',\n",
       " 'excited',\n",
       " 'for',\n",
       " 'the',\n",
       " 'next',\n",
       " '3',\n",
       " 'weeks',\n",
       " 'of',\n",
       " 'my',\n",
       " 'life']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irony[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Emoji Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Emoji raw dadta set, tokenize, and write to an interim csv file\n",
    "emoji = token_data(EMOJI_RAW_PATH + TRAIN, EMOJI_INTERIM_PATH+TRAIN_INTERIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sunday',\n",
       " 'afternoon',\n",
       " 'walking',\n",
       " 'through',\n",
       " 'Venice',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'with',\n",
       " '@user',\n",
       " 'Abbot',\n",
       " 'Kinney',\n",
       " 'Venice']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare tokeniser’s output \n",
    "With the baseline tokenisation from the socialmedia tokeniser in the NLTK library (nltk.tokenize.TweetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open emoji data, tokenize with NLTK tokenizer, write to interim csv for comparison\n",
    "compare_emoji = token_data(EMOJI_RAW_PATH + TRAIN, EMOJI_INTERIM_PATH+'train_seperated_compared.csv',\"compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['️', '️', '️', '️', '@', 'Toys', '\"', 'R', '\"', 'Us']\n",
      "['Toys', 'R', 'Us']\n"
     ]
    }
   ],
   "source": [
    "print(compare_emoji[3])\n",
    "print(emoji[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open emoji data, tokenize with NLTK tokenizer, write to interim csv for comparison\n",
    "compare_irony = token_data(IRONY_RAW_PATH + TRAIN, IRONY_INTERIM_PATH+'train_seperated_compared.csv',\"compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@user', '@user', 'So', 'is', 'he', 'banded', 'from', 'wearing', 'the', 'clothes', '?', '#Karma']\n",
      "['@user', '@user', 'So', 'is', 'he', 'banded', 'from', 'wearing', 'the', 'clothes', '?', '#Karma']\n"
     ]
    }
   ],
   "source": [
    "print(compare_irony[3])\n",
    "print(irony[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterize Data: Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_df = pd.DataFrame(irony)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irony Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2862 tweets in the Irony data set\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {irony_df.shape[0]} tweets in the Irony data set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_irony = Counter()\n",
    "for line in final_irony:\n",
    "    count_irony.update(line)\n",
    "\n",
    "# 10 most common tokens in Irony data set\n",
    "count_irony.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_irony_1 = 0\n",
    "counter_irony_ = 0\n",
    "for k,v in count_irony.items():\n",
    "    if v == 1:\n",
    "        counter_irony_1 += 1\n",
    "    elif v > 1:\n",
    "        counter_irony_ += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of words that occur only once: {counter_irony_1}, ({counter_irony_1/(counter_irony_1 + counter_irony_):.0%})\\nnumber of words that occur more than once: {counter_irony_}, \\ntotal word count, including repeated words: {sum(count_irony.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_irony.most_common()[:-10-1:-1] # 10 least common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipf_law(emoji, 'Emoji');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipf_law(irony, 'Irony');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {emoji_df.shape[0]} Tweets in the Emoji data set.\") #should be 45000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_emoji = Counter()\n",
    "\n",
    "for line in final_emoji:\n",
    "    count_emoji.update(line)\n",
    "\n",
    "# 10 most common tokens in emoji dataset\n",
    "count_emoji.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_emoji_1 = 0\n",
    "counter_emoji_ = 0\n",
    "for k,v in count_emoji.items():\n",
    "    if v == 1:\n",
    "        counter_emoji_1 += 1\n",
    "    elif v > 1:\n",
    "        counter_emoji_ += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Common Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of words that occur only once: {counter_emoji_1}, ({counter_emoji_1/(counter_emoji_1 + counter_emoji_):.0%})\\nnumber of words that occur more than once: {counter_emoji_} \\ntotal word count, including repeated words: {sum(count_emoji.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_emoji['# #'], count_emoji['##']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_emoji.most_common()[:-10-1:-1] # Least common words are often personal names or hashtags."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
